{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from content_embedding.important_word_embedding import get_word_count_vncorenlp, tokenize\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# For transformers v4.x+:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'HN', '-', 'Phú_Quốc', ']', 'ROAM', '2021', ':', 'Combo', '3N2Đ', 'Vinpearl', '+', 'VMB', 'Vietnam_Airlines', 'khứ_hồi', '+', 'Ăn', 'sáng', 'hoặc', 'ăn', '3', 'bữa', 'mỗi', 'ngày']\n"
     ]
    }
   ],
   "source": [
    "# For transformers v3.x:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "title = '[HN - Phú Quốc] ROAM 2021: Combo 3N2Đ Vinpearl + VMB Vietnam Airlines khứ hồi + Ăn sáng hoặc ăn 3 bữa mỗi ngày'\n",
    "word_cnt = tokenize(title)\n",
    "print(word_cnt)\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "sentence = 'Chúng_tôi là những nghiên_cứu_viên .'\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(word_cnt)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 26, 768])\n",
      "torch.Size([1, 768])\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 5.1279e-01, -5.1798e-01, -1.0874e+00,  ..., -3.4440e-02,\n",
      "           1.1951e-01, -5.6784e-01],\n",
      "         [-4.4168e-01,  1.6312e-01, -1.9595e-01,  ...,  8.7471e-03,\n",
      "          -2.1305e-01, -4.0150e-01],\n",
      "         [ 3.3823e-01,  4.9155e-01, -5.3765e-01,  ...,  1.4713e-01,\n",
      "           3.0208e-01,  1.9725e-01],\n",
      "         ...,\n",
      "         [ 3.1962e-01,  2.6513e-01, -6.5674e-01,  ..., -3.6543e-01,\n",
      "          -1.0867e-03, -4.5310e-01],\n",
      "         [ 2.9763e-02, -7.0160e-02, -2.6094e-01,  ..., -5.0176e-01,\n",
      "          -9.6884e-02,  1.1561e-01],\n",
      "         [ 4.4358e-01, -4.6743e-01, -1.0138e+00,  ...,  1.3200e-02,\n",
      "          -7.7081e-02, -5.3229e-01]]]), pooler_output=tensor([[ 2.1375e-01,  2.4023e-01, -5.3688e-02,  7.8188e-02,  7.1065e-02,\n",
      "          5.5739e-03, -1.5261e-01,  1.4837e-02,  3.2352e-01, -2.1674e-01,\n",
      "         -1.0965e-02,  2.8101e-02, -5.0831e-02, -1.0354e-01,  2.3707e-01,\n",
      "          4.1540e-02, -3.2124e-01,  7.0423e-02, -2.4001e-01, -9.3955e-02,\n",
      "          2.0009e-01, -2.3407e-01, -2.3712e-02, -6.2424e-02,  9.6168e-02,\n",
      "         -1.4269e-01, -1.0106e-02,  4.6217e-01, -6.4233e-02, -1.0362e-01,\n",
      "          5.3228e-02, -1.2523e-01,  1.2610e-01,  2.1895e-01,  8.2473e-02,\n",
      "         -2.1421e-01, -6.5912e-02,  3.1186e-02,  2.8682e-01, -7.0711e-02,\n",
      "          3.9649e-01,  2.7430e-01,  2.5340e-03, -2.0005e-01, -6.2259e-02,\n",
      "          6.3746e-02,  2.7565e-01,  1.9320e-01, -4.2753e-02,  1.0571e-01,\n",
      "          3.7839e-01, -2.3155e-02,  3.6953e-01,  4.3325e-02, -6.4372e-02,\n",
      "         -1.9874e-01,  8.2358e-02,  5.0421e-01,  1.2162e-01,  1.9213e-01,\n",
      "          8.9300e-02,  2.3587e-01, -6.1150e-02,  2.0552e-01,  1.3772e-01,\n",
      "         -9.5309e-03,  1.7749e-01,  9.0300e-03,  2.3296e-02,  2.8808e-01,\n",
      "          1.9263e-01, -3.7977e-01,  1.2352e-01, -1.6411e-01,  3.2640e-03,\n",
      "          1.8528e-01, -6.6968e-02,  1.4179e-01, -1.1844e-01, -7.2474e-02,\n",
      "          1.9775e-01,  1.9604e-01, -1.7109e-01,  2.4247e-01, -3.5038e-01,\n",
      "          1.1156e-01,  1.1430e-01, -3.9293e-01,  7.4965e-02,  3.9881e-02,\n",
      "          1.0454e-01, -4.2831e-02,  9.6767e-02, -9.8781e-02, -2.6455e-01,\n",
      "          2.7966e-02, -2.7797e-01, -2.4395e-01, -2.8863e-03,  1.4977e-01,\n",
      "         -1.6200e-01,  1.6804e-01, -1.9405e-01, -3.2972e-01,  7.2603e-02,\n",
      "         -2.4467e-01,  4.7595e-02,  4.5042e-02,  1.4556e-01, -2.8036e-01,\n",
      "         -9.4504e-02, -2.4236e-01,  1.4056e-01,  3.4919e-01, -5.8269e-02,\n",
      "          1.2172e-01,  8.7773e-02,  1.2542e-01, -3.8119e-01, -1.0279e-01,\n",
      "          4.9302e-02, -1.0733e-01,  2.2863e-01,  1.5651e-01, -3.9871e-01,\n",
      "          1.6005e-01, -3.9133e-01, -9.3389e-02,  4.1845e-01,  9.5953e-02,\n",
      "          1.4450e-01,  3.9214e-02,  1.5674e-01, -1.8812e-01,  1.9711e-01,\n",
      "         -3.1068e-01, -1.7160e-01,  2.0254e-01,  1.5683e-01,  7.7633e-02,\n",
      "          3.2846e-01,  2.2938e-01, -2.9446e-01, -4.2334e-02,  2.0392e-01,\n",
      "          5.4608e-02, -2.6700e-01, -3.7053e-01, -6.5728e-02,  1.1283e-01,\n",
      "         -2.6261e-01,  4.3762e-02, -3.1027e-01,  3.3136e-01, -3.6864e-01,\n",
      "         -4.3577e-01,  1.0359e-01,  1.3678e-01, -1.2212e-01, -1.0694e-01,\n",
      "         -2.9830e-01,  2.8812e-01, -3.7622e-01, -2.8490e-01, -1.7666e-01,\n",
      "          8.8435e-02, -1.5650e-02,  4.0987e-02,  8.4083e-02, -9.7824e-02,\n",
      "         -1.6698e-01, -2.8779e-02, -1.7492e-01,  1.4416e-02, -2.1672e-01,\n",
      "         -1.1822e-01,  2.1168e-02, -1.9695e-01, -1.1765e-01,  1.4149e-01,\n",
      "          1.5933e-01,  2.5860e-01,  6.3444e-02,  1.8412e-02, -8.3247e-03,\n",
      "          9.1990e-02,  1.9004e-01, -2.3804e-01,  5.8979e-02, -3.3155e-01,\n",
      "         -7.9386e-02,  4.8135e-02,  7.8204e-03, -5.5255e-03, -3.0382e-01,\n",
      "         -3.6597e-02, -2.4968e-02, -1.4613e-01,  5.3778e-02, -3.2787e-01,\n",
      "          5.5975e-02,  1.7834e-01,  2.3516e-01,  4.9477e-02,  1.0451e-01,\n",
      "         -3.5370e-01, -2.3599e-01,  1.0896e-01,  2.7667e-01,  1.4901e-01,\n",
      "         -4.6543e-01,  4.3879e-02, -2.2489e-01, -8.1547e-02,  3.6527e-02,\n",
      "          2.7949e-01,  2.0531e-01,  1.5050e-01,  1.0338e-03,  1.4349e-01,\n",
      "         -1.9511e-02,  8.3723e-02,  8.5512e-02, -1.1887e-01,  1.3630e-01,\n",
      "          3.5955e-01,  1.2208e-01, -2.1495e-01,  5.4604e-02, -1.3984e-01,\n",
      "         -3.0676e-01, -7.6307e-02, -3.6156e-01, -1.3158e-01, -1.2065e-01,\n",
      "         -4.6563e-01, -2.6213e-02, -1.0108e-01,  1.5876e-01, -1.5388e-01,\n",
      "         -4.4672e-02, -2.6386e-01,  1.5976e-01, -1.9510e-01,  1.2487e-01,\n",
      "         -4.8004e-02, -6.6401e-02, -7.7604e-02,  7.3413e-02, -4.0829e-03,\n",
      "          2.5954e-01,  1.0072e-02,  2.9962e-03,  4.4580e-01, -3.3932e-03,\n",
      "         -3.6318e-01, -1.6630e-01,  9.8659e-02,  1.4486e-01,  2.1928e-01,\n",
      "         -1.1036e-01, -3.9857e-01,  1.6020e-02,  1.5059e-01, -2.0596e-01,\n",
      "         -6.3332e-02,  1.3416e-01,  1.0495e-01, -7.4990e-02,  2.3884e-01,\n",
      "         -9.8247e-02, -2.3637e-01,  2.9112e-01, -1.3313e-01,  2.5082e-03,\n",
      "          4.7667e-02,  6.2894e-02, -7.3669e-02,  1.2406e-01,  3.5794e-03,\n",
      "          1.3949e-01,  1.3301e-02,  1.8599e-01,  2.3669e-01,  3.2682e-01,\n",
      "          1.8008e-02, -1.4021e-01,  1.2670e-01, -1.4462e-02,  5.0367e-02,\n",
      "         -1.0443e-01, -3.1433e-02,  2.1882e-02, -1.2058e-01,  1.2576e-01,\n",
      "         -1.7052e-01, -2.6711e-01, -1.5678e-03,  1.1223e-01,  8.5363e-02,\n",
      "         -6.0989e-01, -3.0163e-02, -1.5545e-01, -1.7949e-01,  9.7844e-02,\n",
      "         -4.6623e-01, -8.7744e-02,  1.7909e-01, -1.3712e-01,  1.1516e-01,\n",
      "         -4.2184e-02,  4.1441e-02,  3.6501e-01, -1.1557e-01, -1.4228e-01,\n",
      "          2.7873e-01,  2.8036e-01,  1.9603e-01, -2.9803e-01, -2.4652e-01,\n",
      "         -1.5874e-01, -6.6837e-02, -1.0615e-01, -8.2012e-02,  9.1787e-02,\n",
      "          4.1773e-02,  1.2813e-01,  1.2389e-01, -2.4720e-01,  3.8326e-01,\n",
      "         -1.0161e-01,  1.1082e-01, -5.6596e-02, -1.2882e-01,  3.8317e-01,\n",
      "         -4.3431e-01, -2.9786e-01, -1.6427e-01,  8.7430e-02, -1.9597e-01,\n",
      "          1.0497e-01, -1.2379e-03,  1.7090e-01,  3.5880e-02, -4.2284e-01,\n",
      "          2.0953e-01,  2.1291e-02,  3.4672e-01,  5.6644e-02,  1.0754e-01,\n",
      "          2.1555e-01, -1.3610e-01,  1.3067e-01,  1.9340e-01,  6.2258e-02,\n",
      "         -3.1149e-01, -6.5296e-02,  3.3453e-01,  1.6240e-01, -1.3394e-01,\n",
      "          1.1423e-01,  3.5971e-01,  1.4919e-01,  4.4904e-01,  1.7132e-01,\n",
      "         -1.3177e-01,  2.5608e-01,  2.5553e-01,  1.0009e-01, -4.6065e-01,\n",
      "          1.4511e-02, -7.9474e-02,  1.3976e-01, -4.1076e-02,  5.0899e-03,\n",
      "          9.7517e-02, -1.4690e-01,  2.1615e-01,  2.9978e-01,  6.7605e-02,\n",
      "          1.1234e-02,  2.4995e-01, -2.8202e-01, -3.4156e-01,  1.0431e-02,\n",
      "         -2.0293e-01,  3.2808e-02,  2.7426e-01, -3.6386e-02,  3.1308e-02,\n",
      "         -2.9405e-01, -8.4819e-02, -9.0521e-03,  1.0943e-01, -3.1700e-01,\n",
      "          3.4877e-01, -1.4047e-01,  2.6345e-01,  1.6367e-01, -1.3234e-01,\n",
      "         -4.7438e-02,  6.5197e-02,  6.3060e-02,  2.2621e-01, -1.0408e-01,\n",
      "         -6.0919e-02,  3.5999e-01, -3.5148e-01, -2.9535e-01,  5.5182e-01,\n",
      "         -1.4159e-02,  1.6442e-01,  3.9694e-01,  1.2989e-01,  9.5980e-04,\n",
      "          2.1891e-01,  3.5785e-01, -2.0262e-01,  1.0055e-01,  3.3094e-01,\n",
      "         -2.3037e-01,  8.8637e-02,  2.1019e-01,  1.4366e-01, -1.3031e-01,\n",
      "         -2.1714e-01, -1.3337e-01,  5.5463e-02,  2.3715e-01,  8.6891e-02,\n",
      "          1.6102e-01,  1.2464e-01, -1.6682e-01, -2.4197e-01,  5.5355e-02,\n",
      "          5.9096e-02, -1.7318e-01,  1.6368e-01,  1.8476e-01,  1.1742e-01,\n",
      "         -4.2098e-02,  3.5057e-02,  1.9522e-01, -2.5726e-02,  3.9380e-01,\n",
      "         -1.6614e-01, -4.8833e-01, -2.5768e-01, -2.8343e-01,  2.7067e-01,\n",
      "          4.8137e-02, -8.2117e-02,  1.6464e-01,  2.3873e-01,  4.1763e-01,\n",
      "         -2.3117e-01, -1.0600e-01,  5.2004e-01,  1.5200e-02, -1.0971e-01,\n",
      "          1.2067e-01,  3.1842e-01,  2.5093e-01, -1.2219e-01,  3.4855e-01,\n",
      "          3.2182e-02, -2.1200e-01,  3.1977e-01,  2.8766e-01, -3.5363e-01,\n",
      "          8.3118e-02, -3.1740e-01, -1.7219e-01, -2.5567e-01,  2.0953e-01,\n",
      "         -9.7099e-02,  7.8299e-02,  4.3585e-02, -1.8083e-03, -2.2520e-01,\n",
      "         -2.4707e-01,  3.1450e-01,  3.1959e-01,  1.3927e-01,  3.9961e-03,\n",
      "         -1.2722e-01,  2.0415e-02,  3.9981e-01,  2.6819e-01, -8.0624e-02,\n",
      "         -5.7483e-01,  2.2941e-01,  2.1689e-02, -3.3850e-01,  1.2300e-01,\n",
      "         -8.2407e-02,  8.6122e-02,  2.1031e-01,  3.3015e-01,  8.7140e-02,\n",
      "         -3.6744e-02,  3.4688e-01, -1.0623e-01,  6.9878e-02,  2.3622e-01,\n",
      "          8.2350e-02,  3.4516e-02,  2.7647e-01,  4.3325e-02,  2.0752e-02,\n",
      "         -2.4218e-01,  2.6580e-01,  3.0262e-01,  3.0340e-01, -4.0461e-01,\n",
      "         -1.5435e-01,  5.0690e-01,  2.8010e-01, -8.3638e-02, -2.6273e-01,\n",
      "         -8.7562e-02,  1.1255e-01,  1.2135e-01,  4.0302e-01,  2.1875e-02,\n",
      "         -1.6489e-01, -7.5027e-05,  8.1591e-02, -1.8963e-01, -3.6161e-01,\n",
      "         -2.3944e-01,  2.5909e-01,  1.6781e-01, -1.3146e-01, -1.8733e-01,\n",
      "          2.4869e-01,  1.0215e-01, -2.1126e-01, -3.6057e-01, -1.7297e-01,\n",
      "         -1.6351e-01,  4.7859e-02,  8.4223e-02, -3.7237e-02,  2.5294e-01,\n",
      "          1.1688e-01,  2.9334e-01,  3.1358e-01, -1.0543e-01, -3.7661e-02,\n",
      "          7.2190e-02,  5.9589e-02, -3.0013e-02, -1.0649e-02,  2.5118e-01,\n",
      "          4.2865e-02,  2.0359e-02,  1.3556e-02, -3.7751e-01, -2.4480e-01,\n",
      "          1.1935e-01, -1.0438e-01,  2.6738e-01, -1.9378e-01,  2.3229e-01,\n",
      "          7.7261e-02, -2.4055e-01, -1.2481e-01,  1.5610e-01, -1.4572e-01,\n",
      "         -1.8483e-01,  4.1700e-02, -2.0495e-01,  3.9703e-02,  3.2952e-01,\n",
      "          5.7491e-02, -1.0729e-01, -1.1998e-01,  6.8873e-03,  1.8503e-01,\n",
      "          3.1796e-01,  2.1318e-01,  8.2780e-02,  2.8662e-01, -3.9081e-02,\n",
      "          5.1935e-01, -7.2651e-02,  1.1052e-03,  5.3191e-01, -1.5833e-01,\n",
      "          1.7970e-01, -3.5758e-01,  8.6059e-02, -1.2322e-01, -1.2333e-02,\n",
      "          2.5342e-01,  1.4520e-01, -4.6546e-02,  1.3932e-01, -3.6174e-03,\n",
      "         -6.6246e-02,  1.2497e-01, -1.9320e-01, -7.2286e-03,  2.7785e-02,\n",
      "         -4.1212e-01,  2.0482e-01,  1.4758e-01,  1.5682e-01,  1.4823e-02,\n",
      "          1.8274e-01, -1.0950e-01,  1.4066e-01,  9.5560e-03,  7.2100e-02,\n",
      "          3.6066e-02, -2.6756e-01,  2.5058e-01, -1.5475e-01,  2.2038e-01,\n",
      "         -1.3772e-01, -1.8326e-01,  9.7178e-02, -2.4154e-01,  9.3997e-02,\n",
      "          9.9682e-02, -2.0587e-02, -2.8199e-01, -1.3451e-01, -3.0916e-01,\n",
      "         -2.3738e-01, -1.3290e-01, -4.1013e-02,  3.4810e-02, -1.2472e-01,\n",
      "         -1.7743e-01,  2.7152e-01, -4.0850e-01, -1.9410e-01, -3.9439e-01,\n",
      "          3.5807e-01,  1.2737e-01, -3.3493e-01,  9.7067e-02, -9.2106e-02,\n",
      "         -2.9387e-02, -9.1437e-02,  1.4635e-01,  8.8740e-02,  2.4839e-01,\n",
      "          1.1921e-01, -1.4390e-01, -4.3354e-03,  1.4673e-01, -7.5572e-02,\n",
      "         -1.4047e-01, -4.9867e-02,  1.5747e-01, -5.3137e-02, -1.0688e-01,\n",
      "         -3.1232e-02, -3.1183e-01, -5.7996e-02, -1.6370e-01, -1.3163e-01,\n",
      "          1.3608e-01, -5.4773e-02, -1.1280e-01, -2.2334e-01,  1.3764e-01,\n",
      "          2.8953e-01,  1.3304e-01,  6.3107e-02, -2.8905e-01, -3.0378e-02,\n",
      "          2.7853e-01, -2.5959e-01, -2.4968e-01, -2.0633e-01,  2.0834e-01,\n",
      "          1.3416e-01, -4.1164e-01,  4.2221e-01, -2.2145e-01,  1.3828e-01,\n",
      "         -7.3251e-02,  7.6166e-02,  1.4718e-01,  3.5805e-02, -1.3532e-01,\n",
      "         -2.5060e-01,  7.9469e-02, -8.8112e-03, -1.1738e-01,  6.2878e-03,\n",
      "          1.2081e-01,  1.4546e-01, -1.2193e-01, -5.9790e-02, -5.4171e-02,\n",
      "          1.6556e-01,  2.5392e-01, -1.3595e-01, -2.1280e-03, -1.5056e-01,\n",
      "         -1.0489e-01,  1.5719e-01, -1.7041e-01,  1.6147e-01,  2.7948e-02,\n",
      "          1.8070e-01, -3.0271e-01,  3.4589e-01,  1.3129e-01, -1.5845e-01,\n",
      "         -9.4169e-02,  2.1596e-02,  8.0529e-02, -1.8158e-02,  2.2133e-03,\n",
      "         -4.4071e-01,  5.9125e-02, -1.9155e-03, -2.3927e-01,  1.7528e-01,\n",
      "          3.2238e-02, -4.2894e-02,  1.4706e-01,  1.8344e-01,  4.1630e-01,\n",
      "          1.3068e-01,  4.1258e-02, -2.0443e-01,  1.7625e-01, -1.8074e-01,\n",
      "          1.3572e-01,  3.0173e-02,  1.7127e-01, -1.5942e-01,  4.2834e-01,\n",
      "          3.4025e-01,  2.4246e-01, -2.8383e-02, -8.4791e-02,  1.7167e-01,\n",
      "          2.6034e-01,  3.0292e-01,  7.4037e-02,  9.6541e-02, -2.0680e-01,\n",
      "         -2.2259e-01,  1.7501e-01,  1.3076e-01, -6.1698e-03,  3.5365e-02,\n",
      "          2.0322e-01, -3.9922e-01, -1.7477e-01,  3.8087e-01, -3.9713e-01,\n",
      "          2.4729e-01, -3.8767e-01,  2.3673e-01,  1.4963e-01,  3.4256e-02,\n",
      "         -1.6557e-01,  1.7102e-01, -7.3561e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    features = phobert(input_ids)  # Models outputs are now tuples\n",
    "    print(len(features[1]))\n",
    "    print(features.last_hidden_state.shape)\n",
    "    print(features.pooler_output.shape)\n",
    "    print(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"trituenhantaoio/bert-base-vietnamese-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"trituenhantaoio/bert-base-vietnamese-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 1, 608, 224, 1, 1992, 9237, 1, 10086, 154, 2831, 1, 8177, 151, 224, 941, 1954, 8145, 1, 577, 6268, 1728, 2725, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m title_token \u001B[38;5;241m=\u001B[39m tokenizer(title, max_length\u001B[38;5;241m=\u001B[39mnum_words_title,\n\u001B[1;32m      5\u001B[0m                 pad_to_max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(title_token)\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/miniconda3/envs/ds/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ds/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1545\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1539\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m   1540\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m   1541\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m   1542\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1543\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1545\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1546\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1547\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1548\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1549\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1550\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1551\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1552\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1553\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1554\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1555\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1557\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   1559\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[0;32m~/miniconda3/envs/ds/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ds/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:944\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    942\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    943\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 944\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m \u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m()\n\u001B[1;32m    945\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    946\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m inputs_embeds\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "num_words_title=24\n",
    "title = '[HN-Phú Quốc] ROAM 2021: Combo 3N2Đ Vinpearl + VMB Vietnam Airlines khứ hồi + Ăn sáng hoặc ăn 3 bữa mỗi ngày'\n",
    "title = title.lower()\n",
    "title_token = tokenizer(title, max_length=num_words_title,\n",
    "                pad_to_max_length=True, truncation=True)\n",
    "print(title_token)\n",
    "print(model(title))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "trituenhantaoio_diacritics_model = BertForSequenceClassification.from_pretrained(\"trituenhantaoio/bert-base-vietnamese-diacritics-uncased\")\n",
    "trituenhantaoio_diacritics_tokenizer = BertTokenizer.from_pretrained(\"trituenhantaoio/bert-base-vietnamese-diacritics-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "title = '[HN-Phú Quốc] ROAM 2021: Combo 3N2Đ Vinpearl + VMB Vietnam Airlines khứ hồi + Ăn sáng hoặc ăn 3 bữa mỗi ngày'\n",
    "title = title.lower()\n",
    "# title_ = trituenhantaoio_diacritics_tokenizer(title)\n",
    "print(trituenhantaoio_diacritics_tokenizer(title))\n",
    "print(trituenhantaoio_diacritics_model(title))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"trituenhantaoio/bert-base-vietnamese-diacritics-uncased\")\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "# bert_model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"trituenhantaoio/bert-base-vietnamese-diacritics-uncased\")\n",
    "\n",
    "title = '[HN-Phú Quốc] ROAM 2021: Combo 3N2Đ Vinpearl + VMB Vietnam Airlines khứ hồi + Ăn sáng hoặc ăn 3 bữa mỗi ngày'\n",
    "print(base_tokenizer(title))\n",
    "print(bert_model(title))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}